[{"text": "\u2713 \u2717\n 1\n2\nPrompt Engineering\n October 10, 2025\nii\nContents\n 1 IntroductiontoPromptEngineeringandLLMs 1\n 1.1 EvolutionofNLPtoLargeLanguageModels . . . . . . . . . . . . . . . . . . . . 1\n 1.1.1 Rule-BasedSystems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n 1.1.2 StatisticalLanguageModels . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n 1.1.3 NeuralLanguageModels. . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n 1.1.4 PretrainedLanguageModels(PLMs). . . . . . . . . . . . . . . . . . . . . 7\n 1.1.5 LargeLanguageModels(LLMs) . . . . . . . . . . . . . . . . . . . . . . . 7\n 1.2 AnatomyofLLMs:Tokens,embeddings,contextwindows . . . . . . . . . . . . . 7\n 1.2.1 Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n 1.2.2 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n 1.2.3 ContextWindows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n 1.2.4 PositionalEncoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n 1.2.5 AttentionMechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n 1.2.6 TokenLimitsandWorkflow . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n 1.2.7 VisualDiagram. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n 1.3 WhyPromptEngineeringMatters . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n 1.4 ApplicationsinAIandMachineLearning . . . . . . . . . . . . . . . . . . . . . . 11\n 1.5 IntroductiontoGPT,Claude,PaLM,Mistral . . . . . . . . . . . . . . . . . . . . 12\n 1.5.1 GPT(GenerativePre-trainedTransformer)\u2013OpenAI . . . . . . . . . . . 12\n 1.5.2 Claude\u2013Anthropic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n 1.5.3 PaLM(PathwaysLanguageModel)\u2013Google/DeepMind . . . . . . . . 13\n 1.5.4 Mistral\u2013MistralAI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n 1.6 Promptingvsfine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n 1.7 Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n 1.8 Prompting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n 1.8.1 Fine-TuningvsPrompting\u2013Comparison . . . . . . . . . . . . . . . . . . 15\n 2 FoundationsofPromptDesign 17\n 2.1 Typesofprompting:Zero-shot,One-shot,Few-shot. . . . . . . . . . . . . . . . . 17\n 2.1.1 What isShot-BasedPrompting? . . . . . . . . . . . . . . . . . . . . . . . 17\n 2.1.2 Zero-ShotPrompting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n 2.1.3 One-ShotPrompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n 2.1.4 Few-ShotPrompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n 2.1.5 HowtoChoosetheRightPromptingTechnique. . . . . . . . . . . . . . . 19\n 2.2 PromptPatternsOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n 2.2.1 InstructionalPromptPattern . . . . . . . . . . . . . . . . . . . . . . . . . 19\n 2.2.2 DelimitingPromptPattern . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n 2.2.3 Role-BasedPromptPattern . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n 2.2.4 SocraticPromptPattern. . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n 2.3 Promptevaluationmetrics: accuracy,relevance,hallucination,safety . . . . . . . 21\n 2.3.1 Relevance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n iii\niv CONTENTS\n 2.3.2 Hallucination. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n 2.4 Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n 2.5 Promptdebuggingtechniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n 3 AdvancedPromptingTechniques 29\n 3.0.1 ChainofThoughtPrompting . . . . . . . . . . . . . . . . . . . . . . . . . 29\n 3.0.2 Self-AskPrompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n 3.0.3 SelfAskPrompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n 3.0.4 Multimodalprompting(text+image) . . . . . . . . . . . . . . . . . . . . 34\n 3.1 PrompttemplatesusingPythonAPIs(LangChain,OpenAISDK). . . . . . . . . 36\n 3.1.1 ComponentsofLangChain . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n 3.2 PromptTemplatesinLangChain . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n 3.2.1 2.ChatPromptTemplates . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n 3.2.2 ExampleOutput. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n 3.3 PromptTemplates+OpenAISDKExamples . . . . . . . . . . . . . . . . . . . . 38\n 3.3.1 1. StringPromptTemplate+OpenAIChatAPI . . . . . . . . . . . . . . 39\n 3.3.2 2.Chat-StylePromptTemplate+OpenAIChatAPI . . . . . . . . . . . 40\n 4 PromptEngineeringforDomain-SpecificTasks 41\n 4.1 CodingAssistantFeatureswithExamples . . . . . . . . . . . . . . . . . . . . . . 41\nList of Figures\n 1.1 Flowchart of the Transformer-based Text Processing in a Language Model . . . . 10\n 2.1 Types of Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n 3.1 Illustration of LangChain components and features. . . . . . . . . . . . . . . . . . 36\n v\nvi\n LIST OF FIGURES\nListofTables\n 1.1 ComparisonBetweenPretrainedLanguageModels(PLMs)andLargeLanguage\n Models(LLMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n 1.2 ComparisonBetweenFine-TuningandPromptinginLargeLanguageModels . . 15\n 2.1 ComparisonofPromptPatternTypesandTheirCharacteristics . . . . . . . . . 22\n 2.2 PromptEvaluationCriteria:Definitions,Considerations,andExamples . . . . . 24\n vii\nviii\n LIST OF TABLES\n1\n Introduction to Prompt Engineering\n and LLMs\n This chapter delves into the transformation of Natural Language Processing (NLP) from its\n early foundations in rule-based systems and statistical methods to the cutting-edge era of Large\n Language Models (LLMs) that drive today\u2019s AI-powered applications. It begins by tracing the\n historical journey\u2014highlighting the limitations of traditional approaches, the breakthroughs\n introduced by neural network architectures, and the revolutionary impact of the Transformer\n model that enabled large-scale, context-aware language understanding.\n Building on this foundation, the chapter examines the anatomy of LLMs, focusing on three\n essential components: tokens, the basic units into which text is segmented; embeddings, the\n dense vector representations that encode semantic meaning; and context windows, which define\n how much information the model can consider at once, directly influencing its ability to handle\n complex reasoning and maintain coherent narratives.\n A significant portion of the discussion centers on prompt engineering\u2014the practice of craft\ning precise and purposeful instructions to elicit optimal responses from LLMs. The chapter\n explains why prompt engineering has become a critical skill, how it can adapt model behavior\n without retraining, and the range of applications it enables, from intelligent assistants and code\n generation to automated summarization and domain-specific content creation.\n The chapter also introduces some of the most influential LLMs in the current AI land\nscape\u2014OpenAI\u2019s GPT series, Anthropic\u2019s Claude, Google\u2019s PaLM, and Mistral\u2014offering a\n high-level comparison of their capabilities, architectures, and intended use cases. Finally, it\n explores the strategic choice between prompting and fine-tuning as methods for customizing\n model behavior, weighing the trade-offs in terms of adaptability, resource requirements, and\n performance in specialized tasks.\n By the end of this chapter, the reader will have a comprehensive understanding of how LLMs\n evolved from earlier NLP systems, how their core mechanisms operate, why prompt engineering\n is central to leveraging their power, and how different adaptation strategies can be applied\n effectively in AI/ML projects.\n 1.1 Evolution of NLP to Large Language Models\n Large Language Models (LLMs) are advanced artificial intelligence systems designed to under\nstand, process, and generate human language with remarkable fluency and contextual aware\nness. Built on deep learning architectures most notably the Transformer they are trained on\n vast datasets comprising books, articles, websites, and other text sources, enabling them to\n learn grammar, facts, reasoning patterns, and even stylistic nuances.\n An LLM\u2019s capability stems from billions (or even trillions) of parameters that encode pat\nterns and relationships within language. These models work by predicting the next word (or\n 1\n2\n 1. INTRODUCTION TO PROMPT ENGINEERING AND LLMS\n token) in a sequence based on the context provided, allowing them to perform a wide variety\n of tasks without explicit task-specific programming. Such tasks include answering questions,\n summarizing documents, translating languages, generating code, creating conversational agents,\n and assisting in research.\n Unlike earlier NLP models, LLMs are not confined to narrow domains or fixed vocabularies.\n Their flexibility comes from general-purpose training, which allows them to adapt to different\n prompts and contexts on the fly. This adaptability has opened new possibilities in education,\n business, creative industries, and scientific research, making LLMs a central technology in the\n current AI revolution.\n While LLMs are often discussed in the context of popular implementations like OpenAI\u2019s\n GPT series, Anthropic\u2019s Claude, Google\u2019s PaLM, or Mistral\u2019s open-source models, the concept\n itself is broader it represents a leap from specialized linguistic tools to versatile, knowledge-rich\n systems capable of interacting with humans in natural and meaningful ways.\n Having understood what LLMs are in general, we will now explore how they came to\n be\u2014tracing the historical evolution of language processing technologies and the breakthroughs\n that enabled the development of modern LLMs.\n 1.1.1 Rule-Based Systems\n Before machine learning became the driving force behind natural language processing, early\n conversational programs were essentially collections of rules and patterns coded by humans.\n These systems operated with fixed behavior: if certain words or structures appeared in user\n input, the system would respond in a pre-determined way. One of the earliest and most famous\n examples is ELIZA (1966), created by Joseph Weizenbaum at MIT. ELIZA\u2019s \u201cDOCTOR\u201d script\n simulated a Rogerian psychotherapist by turning user statements into questions, giving the\n illusion of understanding. A few years later, PARRY (1972) was created by psychiatrist Kenneth\n Colby to simulate a patient with paranoid schizophrenia. These programs were groundbreaking\n for their time because they showed that computers could appear conversational, even though\n there was no true comprehension happening.\n Rule-based systems rely entirely on explicitly programmed rules to determine responses.\n They operate much like a decision tree:\n 1. Keyword Matching: The system searches the user\u2019s input for specific words or phrases.\n For example, if the word \u201cweather\u201d appears, the chatbot might respond with \u201cWould you\n like to know today\u2019s forecast?\u201d\n 2. Pattern Matching: Beyond individual keywords, many systems use templates with place\nholders for variable parts of the input. These are often written in a form like \"I am feeling\n *X\" where *X can match any word or phrase the user types.\n 3. Response Templates: Once a match is found, the system selects one of several fixed\n responses tied to that pattern. Sometimes there are multiple options, chosen randomly to\n reduce repetition.\n 4. Pronoun Substitution: To make responses sound more natural, some systems swap pro\nnouns in the input to fit the reply \u2014 changing \u201cI\u201d to \u201cyou\u201d or \u201cmy\u201d to \u201cyour.\u201d This made\n ELIZA seem more conversational even though it was mechanically substituting words.\n Unlike modern AI systems, these rule-based programs have no ability to learn from experi\nence. Their \u201cknowledge\u201d is only as broad as the rules the human programmer wrote. If a topic\n isn\u2019t covered by a rule, the system can\u2019t address it meaningfully. For example, if you ask about\n \u201cspace travel\u201d but no rule matches \u201cspace\u201d or \u201ctravel,\u201d the chatbot will either produce a generic\n fallback response like \u201cTell me more about that\u201d or fail entirely. There is no statistical model\ning, probability estimation, or pattern generalization \u2014 it\u2019s just direct mapping from detected\n1.1. EVOLUTION OF NLP TO LARGE LANGUAGE MODELS\n 3\n pattern to canned output. This makes the system predictable but also extremely limited. There\n was no statistical prediction. If you typed \u201cI am feeling\u201d, the system wouldn\u2019t predict \u201chappy\u201d\n \u2014it would search for the keyword \u201cfeeling\u201d in its script and reply with a fixed sentence like\n \u201cWhy are you feeling that way?\u201d It\u2019s deterministic, not probabilistic. Example: User: \u201cI am\n feeling very\u201d System: (searches for \u201cfeeling\u201d) \u2192 \u201cWhy are you feeling that way?\u201d\n 1.1.2 Statistical Language Models\n After the era of rule-based systems, the next big step in natural language modeling came with\n statistical methods. Instead of hardcoding patterns, researchers began learning patterns from\n large text corpora by counting how often sequences of words occurred. This was the foundation\n of statistical language modeling and was widely used in:\n 1. Early speech recognition (e.g., Dragon NaturallySpeaking, early Google Voice Search)\n 2. Machine translation (e.g., early Google Translate)\n 3. Text prediction (e.g., T9 keyboards, early phone autocorrect)\n 4. Information retrieval (search engines estimating query likelihood)\n This period represented the shift from handwritten logic to data-driven probability models.\n A Statistical Language Model (SLM) is a probability distribution over sequences of words.\n It helps us estimate how likely a particular sentence is in a language.\n In simple terms:\n 1. It answers \u201cHow likely is this sentence to occur in real language?\u201d\n 2. It is widely used in speech recognition, machine translation, text generation, spelling\n correction, autocomplete, and chatbots.\n The basic idea is For a sequence of words W = (w1,w2,...,wn):\n P(W) =P(w1,w2,...,wn)\n Using the chain rule of probability:\n P(W) =P(w1)\u00d7P(w2|w1)\u00d7P(w3|w1,w2)\u00d7\u00b7\u00b7\u00b7\u00d7P(wn|w1,...,wn\u22121)\n Since exact computation is difficult, approximations such as n-gram models are used.\n Types of SLM\n Depend on how much previous words the next word is predicted there are types of n-gram\n models.\n 1. Unigram Model:- Assumes each word is independent.\n n\n P(w1,w2,...,wn) =\n i=1\n Example: Corpus: \u201cI like apples. I like bananas.\u201d\n Counts:\n P(wi)\n P(I) = 2\n 6 = 0.33, P(like) = 2\n 6 = 0.33, P(apples) = 1\n 6 = 0.17\n Sentence: \u201cI like apples\u201d\n P =0.33\u00d70.33\u00d70.17 = 0.018\n4\n 1. INTRODUCTION TO PROMPT ENGINEERING AND LLMS\n 2. Bigram Model:- Each word depends on the previous word.\n n\n P(w1,...,wn) = P(w1)\u00d7\n Example: From corpus:\n i=2\n P(wi|wi\u22121)\n P(like|I) = 2\n 2 = 1.0, P(apples|like) = 1\n 2 = 0.5\n Sentence: \u201cI like apples\u201d\n Source\u2013Channel Models\n P =0.33\u00d71.0\u00d70.5 = 0.165\n These models treat language generation as a noisy communication process: a source produces a\n sentence W with probability P(W) and a channel transmits it with probability P(A|W) given\n an observation A.\n Example: In Automatic Speech Recognition (ASR), given acoustic signal A, the model selects\n the word sequence W\u2217 = argmaxW P(W)\u00b7P(A|W).\n Generative Models\n Generative models attempt to simulate the process of sentence generation and capture long\ndistance dependencies.\n \u2022 Higher-order n-grams: Extend beyond trigram, but require massive data.\n \u2022 Skipping models: Predict words while skipping function words.\n Example: Predicting \u201capples\u201d based on \u201cI\u201d and \u201clike\u201d, ignoring intervening function\n words.\n \u2022 Grammar-based models: Extend P(W) to P(W,T), where T is a parse tree. Example:\n Subject\u2013verb agreement across clauses is better captured.\n \u2022 Dependency-based models: Use word-to-word dependencies instead of full trees. Ex\nample: \u201ceat \u2192 apples\u201d helps predict the object of the verb.\n Discriminative Models\n Discriminative models focus on ranking or classifying candidate sequences directly, instead of\n modeling the full distribution. They often use linear or maximum-entropy models with features.\n Example: In ASR, the model ranks candidate outputs like \u201cI like apples\u201d vs. \u201cEye like apples\u201d\n by using both acoustic and linguistic features.\n Grammar-based Hybrid Models\n These combine n-gram models with syntactic grammars such as Stochastic Context-Free Gram\nmars (SCFG).\n Example: A trigram captures local context, while SCFG ensures long-distance constraints like\n subject\u2013verb agreement.\n1.1. EVOLUTION OF NLP TO LARGE LANGUAGE MODELS\n 5\n Application-specific Models\n Specialized variants of SLMs are tailored for particular tasks.\n \u2022 Cross-lingual SLM: Uses lexical triggers or latent semantic indexing to transfer knowl\nedge from a resource-rich language (e.g., English) to a low-resource one.\n \u2022 Spoken Document Retrieval Models: Use both word- and syllable-level models for\n languages like Mandarin.\n \u2022 Summarization Models: Treat summarization as sentence compression.\n Example: \u201cI really like to eat apples.\u201d \u2192 \u201cI like apples.\u201d\n In the early 1970s, the field of artificial intelligence saw the introduction of the Hidden\n Markov Model (HMM) by Leonard E. Baum (1971) et al., and Conditional Random Fields\n (CRFs).\n HMMs use probabilities to figure out what\u2019s happening in a sentence, like identifying the\n role of a word (noun, verb, etc.). They\u2019re really good at handling sequences of words and finding\n the most likely story behind a sentence, making them useful for tasks like speech recognition\n and part-of-speech tagging.\n 1.1.3 Neural Language Models\n The evolution of neural network architectures has played a pivotal role in shaping the field\n of natural language processing (NLP). These developments, from the Perceptron to modern\n recurrent architectures, have expanded the ability of machines to process and understand human\n language.\n Convolutional Neural Networks (CNNs)\n In the 1990s, Convolutional Neural Networks (CNNs) were introduced. While CNNs\n are primarily associated with image processing, they have also been applied to NLP tasks\n such as text classification, sentence modeling, and feature extraction from word sequences. By\n applying convolutional filters over word embeddings, CNNs can capture local features and n\ngram patterns in text.\n Example: In sentiment analysis, a CNN could detect phrases like \u201cvery good\u201d or \u201cnot\n happy\u201d to determine the sentiment of a review.\n Recurrent Neural Networks (RNNs)\n Introduced in 1986, Recurrent Neural Networks (RNNs) are designed to handle sequential\n data. RNNs can capture short-term dependencies by passing information from one time step to\n6\n 1. INTRODUCTION TO PROMPT ENGINEERING AND LLMS\n the next. However, they struggle with long-range dependencies due to the vanishing gradient\n problem.\n Next-Word Prediction Example Suppose the input sequence is:\n x =[The, cat, is, on, the]\n An RNN predicts the next word based on the previous context. Potential predictions:\n \u2022 \u201cmat\u201d (most likely)\n \u2022 \u201croof\u201d (less likely)\n \u2022 \u201ctable\u201d (possible)\n Recurrent Neural Network Language Model (RNNLM)\n In 1991, Elman developed the RNN Language Model (RNNLM), which excels at modeling\n short-term word relationships within sequences but struggles with long-range dependencies.\n Latent Semantic Analysis (LSA)\n Introduced by Landauer and Dumais in 1997, Latent Semantic Analysis (LSA) represents\n words and documents in a high-dimensional semantic space. LSA captures conceptual similar\nities between words and documents.\n Example: LSA might identify that \u201ccar\u201d and \u201cautomobile\u201d are semantically related even\n if they rarely appear together.\n Long Short-Term Memory (LSTM) Networks\n A major breakthrough came in 1997 with Long Short-Term Memory (LSTM) networks.\n LSTMs use gates (input, forget, output) to regulate information flow, allowing them to capture\n long-term dependencies.\n Next-Word Prediction Example Input sequence:\n x =[The, weather, today, is, very]\n LSTM prediction probabilities:\n \u2022 \u201csunny\u201d (high probability)\n \u2022 \u201ccold\u201d (medium probability)\n \u2022 \u201chappy\u201d (low probability)\n Gated Recurrent Units (GRUs)\n In 2014, Gated Recurrent Units (GRUs) were proposed as a simplified alternative to\n LSTMs. GRUs use fewer parameters but maintain strong performance in capturing sequen\ntial patterns.\n Next-Word Prediction Example Input sequence:\n x =[Artificial, intelligence, is, transforming]\n GRU prediction probabilities:\n \u2022 \u201ctechnology\u201d (most probable)\n \u2022 \u201ceducation\u201d (less probable)\n \u2022 \u201chealthcare\u201d (less probable)\n1.2. ANATOMY OF LLMS: TOKENS, EMBEDDINGS, CONTEXT WINDOWS\n 7\n 1.1.4 Pretrained Language Models (PLMs)\n Pretrained Language Models (PLMs) are models trained on massive text corpora in a self\nsupervised fashion. They capture linguistic features such as syntax, semantics, and context.\n Once pretrained, these models can be fine-tuned on specific downstream tasks (e.g., machine\n translation, summarization, sentiment analysis).\n Examples: Word2Vec, GloVe, ELMo, BERT, GPT (early versions).\n *Illustrative Example Given the sentence:\n The student is reading a ___\n A PLM predicts possible next words such as \u201cbook\u201d, \u201cpaper\u201d, or \u201cnovel\u201d. During fine-tuning,\n if the model is trained for a medical domain, it might predict \u201cjournal\u201d or \u201carticle\u201d with higher\n probability.\n 1.1.5 Large Language Models (LLMs)\n Large Language Models (LLMs) are an extension of PLMs with billions or trillions of parameters.\n Their scale enables them to generalize across a wide range of tasks without explicit task-specific\n f\n ine-tuning. Instead, they can perform zero-shot, one-shot, or few-shot learning simply by\n conditioning on prompts.\n Examples: GPT-3, GPT-4, PaLM, LLaMA, Claude.\n *Illustrative Example Given the prompt:\n Translate: \u201cHow are you?\u201d to French.\n An LLM responds directly with \u201cComment \u00e7a va ?\u201d without requiring task-specific retraining.\n Comparison Between PLMs and LLMs\n Table 1.1: Comparison Between Pretrained Language Models (PLMs) and Large Language\n Models (LLMs)\n Aspect\n Pretrained Language Models (PLMs) Large Language Models (LLMs)\n Training Objective Pretraining + Fine-tuning\n Pretraining, often used directly\n Scale\n Millions to billions of parameters\n Adaptability\n Billions to trillions of parameters\n Requires fine-tuning for new tasks\n Can adapt with prompting (zero/few-shot)\n Examples\n Capabilities\n Word2Vec, GloVe, BERT, GPT-2\n GPT-3, GPT-4, PaLM, LLaMA\n Strong in specific tasks after fine-tuning\n General-purpose reasoning, text generation\n 1.2 Anatomy of LLMs: Tokens, embeddings, context windows\n Large Language Models (LLMs) like GPT-5 are sophisticated neural networks trained on mas\nsive amounts of text. They understand and generate human language by processing it through\n multiple stages. The key components of an LLM include:\n \u2022 Tokens: The basic units of text.\n \u2022 Embeddings: Numerical vector representations of tokens.\n \u2022 Context Windows: How much text the model can consider at once.\n \u2022 Positional Encoding: Allows the model to understand the order of tokens.\n8\n 1. INTRODUCTION TO PROMPT ENGINEERING AND LLMS\n \u2022 Attention Mechanism: Helps the model focus on relevant tokens when predicting the\n next token.\n \u2022 Token Limits and Workflow: How the model processes inputs and generates outputs.\n 1.2.1 Tokens\n Tokens are the smallest units of text that the model processes. They allow LLMs to handle\n text in a structured way.\n Types of Tokens:\n \u2022 Words: Complete words like \u201ccat\u201d or \u201chouse\u201d.\n \u2022 Subwords: Parts of words, useful for rare or compound words. Example: \u201cun\u201d, \u201cbreak\u201d,\n \u201c-able\u201d.\n \u2022 Characters: Individual letters or symbols (less common in modern LLMs).\n Tokenization Methods:\n \u2022 Byte Pair Encoding (BPE): Merges frequently occurring character sequences.\n \u2022 SentencePiece: Handles multilingual and unknown words efficiently.\n Example:\n Text: \"ChatGPT is amazing.\"\n Tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" amazing\", \".\"]\n Tokens are crucial because they are the input units for embeddings.\n 1.2.2 Embeddings\n Embeddings convert tokens into numerical vectors that the model can process.\n Characteristics:\n \u2022 Dense vectors of hundreds or thousands of dimensions.\n \u2022 Capture semantic meaning: similar words have similar embeddings.\n \u2022 Learned during training via backpropagation.\n Example:\n Token: \"cat\" \u2192 Embedding: [0.12,\u22120.34,0.87,...,0.45]\n Token: \"dog\" \u2192 Embedding: [0.15,\u22120.30,0.89,...,0.41]\n Embeddings are the foundation for the model\u2019s ability to reason and understand language\n mathematically.\n 1.2.3 Context Windows\n The context window determines the maximum number of tokens a model can process at one\n time. Modern large language models (LLMs) support context windows of varying sizes, ranging\n from 8,000 to 32,000 tokens, and in some cases even up to 128,000 tokens. Any tokens beyond\n this limit are truncated and do not influence the model\u2019s output. The size of the context\n window is crucial for maintaining coherent conversations and enables the model to perform\n reasoning over lengthy documents, ensuring that relevant information is preserved within the\n active context.\n1.2. ANATOMY OF LLMS: TOKENS, EMBEDDINGS, CONTEXT WINDOWS\n 9\n 1.2.4 Positional Encoding\n Neural networks cannot inherently know the order of tokens. Positional encoding solves this.\n \u2022 Adds a unique vector to each token embedding based on its position.\n \u2022 Can be sinusoidal or learned during training.\n Example: Token embeddings for \u201cI love AI\u201d might be modified as:\n EI +P1, Elove +P2, EAI+P3\n This allows the model to distinguish \u201clove AI\u201d from \u201cAI love\u201d.\n 1.2.5 Attention Mechanism\n Attention allows the model to focus on relevant tokens in the context window.\n \u2022 Calculates a weighted importance for each token relative to others.\n \u2022 Essential for capturing long-range dependencies in text.\n Self-Attention Example: For the sentence \u201cThe cat sat on the mat,\u201d the model can focus\n more on \u201ccat\u201d when predicting \u201csat.\u201d\n 1.2.6 Token Limits and Workflow\n Workflow:\n 1. Text Input: Raw text provided by the user.\n 2. Tokenization: Split text into tokens.\n 3. Embedding: Convert tokens to vectors.\n 4. Add Positional Encoding: Incorporate token order.\n 5. Attention Layers: Compute relevance across all tokens.\n 6. Feed-Forward Layers: Transform embeddings further.\n 7. Output Token Prediction: Generate the next token or sequence.\n Token Limit Considerations:\n \u2022 Tokens beyond the context window are ignored.\n \u2022 Very long documents may need summarization or splitting into chunks.\n 1.2.7 Visual Diagram\n LLMs work by transforming text into tokens, embedding those tokens into numerical vectors,\n understanding their positions, applying attention to relevant parts of the context, and generating\n outputs. Every component\u2014tokens, embeddings, context windows, positional encoding, and\n attention\u2014plays a crucial role in making modern language models capable of understanding\n and generating coherent human language.\n10\n 1. INTRODUCTION TO PROMPT ENGINEERING AND LLMS\n Text Input\n Tokenization\n Embedding\n Add Positional Encoding\n Attention Mechanism\n Feed-Forward Layers\n Output Tokens/Text\n Figure 1.1: Flowchart of the Transformer-based Text Processing in a Language Model\n 1.3 Why Prompt Engineering Matters\n Prompt engineering is the practice of carefully designing the input instructions given to a\n language model to elicit the most accurate, relevant, and useful output. It matters because\n large language models (LLMs) don\u2019t inherently \u201cknow\u201d what you want\u2014they generate responses\n based on patterns learned from training data, so the way you phrase your question or instruction\n directly shapes the quality, clarity, and reliability of the model\u2019s response. A well-crafted prompt\n can dramatically improve performance, helping the model reason logically, follow multi-step\n instructions, produce contextually appropriate content, and reduce errors or hallucinations,\n while a poorly worded prompt can lead to vague, irrelevant, or even misleading answers. Prompt\n engineering is crucial across multiple dimensions\n 1. Accuracy and Reliability:Clear, specific prompts reduce ambiguity, improving correct\nness and minimizing hallucinations. Adding context, examples, or explicit instructions\n guides the model toward desired reasoning paths.\n 2. Creativity and Style Control:By specifying tone, format, or perspective in the prompt,\n you can generate outputs that match a particular writing style, audience, or creative\n requirement.\n 3. Efficiency and Cost: Optimized prompts can achieve high-quality results with fewer\n tokens or iterations, saving both time and computational resources.\n 4. Complex Reasoning and Multi-step Tasks: Breaking tasks into structured prompts,\n like chain-of-thought or step-by-step instructions, helps models handle intricate reasoning\n or multi-layered problems more effectively.\n 5. Context Management: Promptengineering ensures that important information is high\nlighted or prioritized, especially within the limits of a model\u2019s context window, so the\n model \u201cremembers\u201d relevant details.\n 6. Safety and Bias Mitigation: Carefully phrased prompts can reduce the likelihood of\n generating harmful, biased, or inappropriate content by framing questions responsibly and\n specifying constraints.\n1.4. APPLICATIONS IN AI AND MACHINE LEARNING\n 11\n 7. Tool and Domain Integration: In specialized applications like coding, legal research,\n medical summaries, or data analysis well engineered prompts enable the model to adapt\n to domain-specific tasks and leverage external tools or datasets more effectively.\n 8. Scalability and Automation: In business, education, or software, prompt engineering\n allows consistent outputs across many queries, which is critical for automated systems\n and reproducible workflows.\n 1.4 Applications in AI and Machine Learning\n 1. Natural Language Processing (NLP)\n 1. Text generation: Articles, summaries, stories, or reports.\n 2. Translation: Context-aware multilingual translations.\n 3. Sentiment analysis: Classifying text tone and emotional content.\n 4. Question answering and chatbots: Extracting accurate responses and maintaining multi\nturn coherence.\n 2. Code Generation and Software Development\n 1. Programming assistance: Converting natural language instructions into code.\n 2. Debugging and optimization: Identifying errors, suggesting improvements, or explaining\n code.\n 3. Documentation generation: Creating comments and documentation automatically.\n 3. Data Science and Analysis\n 1. Data summarization: Extracting key insights from structured and unstructured data.\n 2. Exploratory analysis: Identifying patterns, trends, and anomalies.\n 3. Feature engineering: Suggesting potential features for machine learning models.\n 4. Knowledge Extraction and Information Retrieval\n 1. Domain-specific question answering: Extracting structured knowledge in legal, medical,\n or research contexts.\n 2. Fact verification: Checking consistency and accuracy of statements.\n 3. Content filtering: Classifying or flagging biased, harmful, or irrelevant content.\n 5. Multi-Modal Applications\n 1. Image and video analysis: Generating captions, scene descriptions, or object recognition.\n 2. Audio processing: Transcription, summarization, or content analysis.\n12\n 1. INTRODUCTION TO PROMPT ENGINEERING AND LLMS\n 6. AI-Assisted Creativity\n 1. Art and design: Generating images, 3D models, or design concepts.\n 2. Music composition: Creating melodies, harmonies, and instrumentation.\n 3. Marketing and copywriting: Crafting targeted ad copy, slogans, and content.\n 7. Automation and Workflow Optimization\n 1. Robotic process automation (RPA): Automating repetitive text-based tasks.\n 2. Decision support: Suggesting actions in business, healthcare, or engineering.\n 3. Process documentation: Generating standard operating procedures and instructional guides.\n 8. Research and Education\n 1. Tutoring and explanations: Teaching concepts or solving problems step-by-step.\n 2. Simulations and hypothesis testing: Exploring \u201cwhat-if\u201d scenarios.\n 3. Knowledge summaries: Condensing scientific literature, technical manuals, or historical\n texts.\n 1.5 Introduction to GPT, Claude, PaLM, Mistral\n 1.5.1\n GPT (Generative Pre-trained Transformer)\u2013 OpenAI\n The GPT series, developed by OpenAI, is one of the most widely known families of large\n language models. Its architecture is a decoder-only transformer that uses causal attention\n to generate text autoregressively. The models are massive in scale, with GPT-3 containing 175\n billion parameters, GPT-3.5 similar, and GPT-4 estimated at over 170 billion parameters.\n Training data for GPT-3 comprised approximately 570GB of filtered text, while GPT-4 was\n trained on a more diverse set of proprietary sources, including text, code, and web data. GPT\n excels in text generation, summarization, reasoning, coding, and Q&A, with GPT-4\n providing advanced reasoning and support for very long contexts (up to 128k tokens in some\n variants). GPT-4 and GPT-4 Turbo also offer multimodal capabilities, handling both\n text and images, and support over 100 languages, making them highly versatile for global\n applications. GPT is widely deployed through APIs, ChatGPT, and developer tools like GitHub\n Copilot.\n 1.5.2\n Claude\u2013 Anthropic\n Claude, developed by Anthropic, is a family of language models designed with a strong em\nphasis on alignment and safety. While specific parameter counts are not publicly disclosed,\n Claude\u2019s architecture is believed to be a decoder-only or hybrid transformer augmented\n with Constitutional AI techniques to guide safe and helpful responses. Claude models are\n trained on proprietary datasets, likely a mix of web and curated texts. Their core capabilities fo\ncus on safe conversational AI, multi-turn reasoning, and structured task completion,\n minimizing harmful or biased outputs. Claude models are primarily text-only and English\nfocused, though Claude 2 includes limited support for other languages. The model series\n includes Claude 1 (Mar 2023), Claude 2 (Jul 2023), and Claude 3 (Mar 2024). They\n are particularly suited for applications requiring high reliability and safety, such as enterprise\n chatbots and sensitive information handling.\n1.6. PROMPTING VS FINE-TUNING\n 13\n 1.5.3\n PaLM (Pathways Language Model)\u2013 Google / DeepMind\n PaLM, developed by Google and DeepMind, is a decoder-only transformer that lever\nages the Pathways architecture, allowing the model to route different tasks to specialized\n subnetworks efficiently. PaLM 2 contains 540 billion parameters and was trained on multi\nterabyte multilingual datasets, designed to excel in reasoning, coding, translation, and\n other NLP tasks. Its architecture enables multitask and multilingual learning, with strong\n performance across over 100 languages. PaLM is text-only, though research versions ex\nplore limited multimodal capabilities. The model is optimized for complex reasoning tasks,\n translation, summarization, and coding, making it particularly strong for multilingual and\n enterprise applications. PaLM 2 was publicly announced in May 2023, representing Google\u2019s\n cutting-edge LLM research.\n 1.5.4\n Mistral\u2013 Mistral AI\n Mistral AI has developed models like Mistral 7B and Mixtral, emphasizing efficiency and\n openness. Mistral 7B is a dense decoder-only transformer with 7 billion parameters, while\n Mixtral is a Mixture-of-Experts (MoE) model with 12.9 billion effective parameters, acti\nvating only a subset of \u201cexperts\u201d per input for computational efficiency. Training data for these\n models is estimated at around 1 trillion tokens, largely proprietary. Mistral models focus on\n text generation, reasoning, and research applications, and are text-only. Multilingual\n support is limited, with most datasets in English. Mistral models are open-weight, allowing\n researchers to fine-tune them for specific tasks, making them ideal for experimental and com\nmercial deployments. Release timelines include Mistral 7B (Sep 2023) and Mixtral (Oct\n 2023).\n 1.6 Prompting vs fine-tuning\n Large Language Models (LLMs) like GPT, Claude, PaLM, and Mistral can be adapted for\n specific tasks in two major ways: fine-tuning and prompting. Both approaches have dis\ntinct methodologies, advantages, limitations, and use cases. This document provides a detailed\n analysis from multiple perspectives.\n 1.7 Fine-Tuning\n Definition\n Fine-tuning is the process of taking a pre-trained language model and further training it on\n a specific dataset or task so that it performs optimally for that task. Unlike pre-training,\n which is broad and general-purpose, fine-tuning adapts the model to specialized needs.\n How It Works\n 1. Start with a pre-trained model: For example, GPT-4 or PaLM 2.\n 2. Prepare task-specific data: Could be question-answer pairs, customer support tickets,\n code snippets, medical notes, or legal documents.\n 3. Adjust weights via gradient descent: The model\u2019s internal parameters are updated\n to minimize error on the new dataset.\n 4. Evaluation and iteration: The fine-tuned model is validated on held-out examples to\n avoid overfitting and ensure generalization.\n14\n 1. INTRODUCTION TO PROMPT ENGINEERING AND LLMS\n Types of Fine-Tuning\n \u2022 Full fine-tuning: All model parameters are updated; requires significant compute.\n \u2022 Parameter-efficient fine-tuning: Only a subset of parameters are trained (e.g., LoRA,\n adapters, prefix tuning); cheaper and faster.\n Advantages\n \u2022 Tailors the model to specific domains or tasks.\n \u2022 Improves accuracy for specialized applications.\n \u2022 Enables creation of customized AI assistants.\n Disadvantages\n \u2022 Computationally expensive for large models.\n \u2022 Requires labeled dataset for the target task.\n \u2022 Can reduce generalization if dataset is too narrow.\n Example Use Cases\n \u2022 GPT-3 fine-tuned on medical literature to assist doctors.\n \u2022 Customer service chatbot trained on a company\u2019s support tickets.\n \u2022 Coding assistant trained specifically on Python and JavaScript repositories.\n 1.8 Prompting\n Definition\n Prompting is the process of giving a pre-trained language model a carefully designed\n input (prompt) to elicit the desired behavior or output. Unlike fine-tuning, the model\n weights are not changed; instead, the model is guided by instructions.\n Key Features of Prompting\n \u2022 Zero-shot prompting: Ask the model to perform a task without examples.\n \u2022 Few-shot prompting: Provide a few examples in the prompt to guide the model.\n \u2022 Instruction-based prompting: Directly instruct the model (e.g., \u201cSummarize this text\n in one paragraph\u201d).\n Advantages\n \u2022 Noadditional training or compute required.\n \u2022 Flexible and fast to adapt to new tasks.\n \u2022 Works with large models out-of-the-box.\n1.8. PROMPTING\n 15\n Disadvantages\n \u2022 Performance may be inconsistent for very specialized tasks.\n \u2022 Requires careful trial-and-error to get optimal prompts.\n \u2022 May not match the accuracy of a fine-tuned model in domain-specific tasks.\n 1.8.1 Fine-Tuning vs Prompting\u2013 Comparison\n Table 1.2: Comparison Between Fine-Tuning and Prompting in Large Language Models\n Aspect\n Fine-Tuning\n Prompting\n Model Modifica\ntion\n Model weights are updated for the\n specific task\n Data\n ment\n Require\nRequires\n Model weights remain unchanged\n labeled\n dataset\n task-specific\n No dataset needed; relies on prompt\n design\n Compute\n Performance\n Expensive, especially for large mod\nels\n Very low; only inference is required\n High for specific task/domain\n Variable; depends on prompt qual\nity\n Flexibility\n Task-specific once fine-tuned\n Highly flexible; can switch tasks in\nstantly\n Time\n Use Cases\n Takes hours to days to fine-tune\n Instant (seconds)\n Specialized applications (medical,\n legal, coding)\n General-purpose, experimentation,\n zero/few-shot tasks\n \u2022 Fine-tuning: Updates model weights to specialize in a task. Offers high accuracy for\n domain-specific tasks but requires compute and data.\n \u2022 Prompting: Uses pre-trained model as-is, guiding behavior through text instructions.\n Flexible and quick but less reliable for specialized tasks.\n16\n 1. INTRODUCTION TO PROMPT ENGINEERING AND LLMS\n2\n Foundations of Prompt Design\n When giving AI models instructions, we can improve their performance by providing examples.\n This technique is called In-Context Learning (ICL). It allows models to learn from examples\n embedded directly in the prompt, rather than needing additional training or fine-tuning. By\n including examples, we guide the AI to better understand the task and expected output, lever\naging its pattern recognition abilities.\n In-Context Learning is especially useful for tasks where instructions alone may not be\n enough, or when a certain structure or style is required in the output. Showing examples\n within the prompt helps the model apply patterns it has learned to similar, unseen inputs.\n Figure 2.1: Types of Prompts\n 2.1 Types of prompting: Zero-shot, One-shot, Few-shot\n 2.1.1 What is Shot-Based Prompting?\n In-Context Learning (ICL) is closely tied to the concept of shot-based prompting methods,\n where \u201cshots\u201d refer to the number of examples included in the prompt.\n Few-shot prompting is a direct application of ICL, where multiple examples (or \u201cshots\u201d)\n are provided to guide the model\u2019s output. The more examples (or shots) we give, the better\n the model typically performs, as it can learn from these examples and generalize them to new,\n similar tasks.\n 17\n18\n 2. FOUNDATIONS OF PROMPT DESIGN\n Common Shot-Based Methods\n \u2022 Zero-Shot Prompting: No examples are provided, and the model must rely entirely on\n its pre-trained knowledge.\n \u2022 One-Shot Prompting: A single example is given to clarify the task for the model.\n \u2022 Few-Shot Prompting: Two or more examples are included, allowing the model to\n recognize patterns and deliver more accurate responses.\n Each of these techniques has strengths depending on the task, and the examples provided\n help the model learn in context, improving accuracy and output quality.\n 2.1.2 Zero-Shot Prompting\n Zero-shot prompting is the simplest form of prompting. Here, we give the model a direct\n instruction to perform a task without providing any examples or demonstrations. This means\n the model has to rely entirely on its pre-trained knowledge to figure out how to complete the\n task.\n Example of Zero-Shot Prompting\n Zero-Shot Prompt\n Classify the sentiment of the following text as positive, negative, or neutral.\n Text: I think the vacation was okay.\n Sentiment:\n AI Output: Neutral\n While zero-shot prompting can work well for simple tasks, it is often not enough for more\n complex tasks. The lack of examples leaves the model guessing, and results can be unpredictable.\n 2.1.3 One-Shot Prompting\n One-shot prompting enhances zero-shot prompting by providing a single example before the\n new task, which helps clarify expectations and improves model performance.\n Example of One-Shot Prompting\n One-Shot Prompt\n Classify the sentiment of the following text as positive, negative, or neutral.\n Text: The product is terrible. Sentiment: Negative\n Text: I think the vacation was okay. Sentiment:\n AI Output: Neutral\n One-shot prompting gives the model a starting point, but with only one example, it might\n still struggle with nuanced or complex tasks. More examples are often needed for better accu\nracy.\n 2.1.4 Few-Shot Prompting\n Few-shot prompting provides two or more examples, which helps the model recognize patterns\n and handle more complex tasks. With more examples, the model gains a better understanding\n of the task, leading to improved accuracy and consistency.\n2.2. PROMPT PATTERNS OVERVIEW\n 19\n Example of Few-Shot Prompting\n Few-Shot Prompt\n Classify the sentiment of the following text as positive, negative, or neutral.\n Text: The product is terrible. Sentiment: Negative\n Text: Super helpful, worth it. Sentiment: Positive\n Text: It doesn\u2019t work! Sentiment:\n AI Output: Negative\n Few-shot prompting helps the model generalize from multiple examples, making it more\n reliable for tasks requiring adherence to specific patterns or formats.\n 2.1.5 How to Choose the Right Prompting Technique\n Selecting the appropriate prompting technique\u2014zero-shot, one-shot, or few-shot\u2014depends on\n the complexity of the task and the level of guidance the model requires.\n Summary of Use Cases\n \u2022 Zero-Shot Prompting: Use when the task is simple, well-understood, or frequently\n encountered. Efficient for basic arithmetic, general queries, or common sentiment classi\nf\n ication.\n \u2022 One-Shot Prompting: Helpful for tasks needing specific guidance or to reduce ambi\nguity. Good for basic classification or structured information extraction.\n \u2022 Few-Shot Prompting: Best for complex tasks requiring multiple examples to establish\n patterns. Ideal for structured outputs or nuanced classifications.\n 2.2 Prompt Patterns Overview\n Prompt patterns are structured methods used to design effective prompts for large language\n models (LLMs) such as ChatGPT or GPT-5. Each pattern shapes how the AI interprets,\n reasons, and responds to your input. The four main types discussed here are the Instructional,\n Delimiting, Role-based, and Socratic patterns. Each has its own working mechanism,\n strengths, and ideal use cases.\n 2.2.1 Instructional Prompt Pattern\n The Instructional prompt pattern is the most direct and commonly used approach for guid\ning AI. It functions by providing a clear, actionable command such as \u201csummarize,\u201d \u201cexplain,\u201d\n \u201clist,\u201d or \u201ccompare.\u201d The AI processes these commands to generate focused outputs that meet\n the stated objectives. This works effectively because large language models are trained to fol\nlow explicit directives. By specifying the audience, tone, and format, users can control how\n the AI structures its output. The major benefits of this pattern include clarity, precision, and\n predictability, making it ideal for academic, professional, and creative writing tasks.\n20\n 2. FOUNDATIONS OF PROMPT DESIGN\n Examples of Instructional Prompts\n \u2022 Write a 200-word report explaining the effects of deforestation on biodiversity.\n \u2022 Summarize the article in three bullet points using simple language.\n \u2022 List five advantages of electric vehicles over traditional cars.\n \u2022 Explain Newton\u2019s three laws of motion as if you were teaching a child.\n \u2022 Generate a professional cover letter for a job application in marketing.\n 2.2.2 Delimiting Prompt Pattern\n The Delimiting prompt pattern improves clarity by using visual or structural boundaries\n such as triple backticks (\u201c\u201c\u2018), quotation marks, or XML-style tags. These delimiters separate\n your instructions from the text or data that the AI must analyze, reducing misinterpretation.\n This technique works because language models read input as a single sequence of text; delimiters\n act as clear markers that define sections. Its main benefits include precision, structure, and\n accuracy, especially when working with long passages, data analysis, or text extraction tasks.\n Examples of Delimiting Prompts\n \u2022 Summarize the following paragraph in one sentence:\n \u201c\u2018Artificial intelligence is revolutionizing industries by improving efficiency and\n reducing human error.\u201c\u2018\n \u2022 Extract all phone numbers from the text below.\u2013- Contact: 9876543210, Office: 022-3456789\u2013\n\u2022 Translate the <text> section into Spanish.\n <text>Education is the most powerful tool to change the\n world.</text>\n \u2022 Correct grammar errors in the passage below.\n \"\"\"He don\u2019t likes to study because the books is boring.\"\"\"\n \u2022 Identify all nouns in the following text:\u2013- The cat sat on the warm window sill.\u2013\n2.2.3 Role-Based Prompt Pattern\n The Role-based prompt pattern assigns a persona or identity to the AI, such as a teacher,\n doctor, lawyer, or artist. This technique guides the model\u2019s tone, vocabulary, and reasoning\n style, helping it simulate expertise or creativity appropriate to that role. It works because\n language models have learned to associate linguistic styles and domain-specific knowledge with\n various professions during training. Its primary benefits are contextual accuracy, realistic tone,\n and improved relevance, making it useful for education, professional consulting, storytelling,\n and simulations.\n2.3. PROMPTEVALUATIONMETRICS:ACCURACY,RELEVANCE,HALLUCINATION,SAFETY21\n Examples of Role-Based Prompts\n \u2022 You are a nutritionist. Suggest a balanced 7-day meal plan for someone trying to\n lose weight.\n \u2022 Act as a historian. Explain the causes and effects of the French Revolution.\n \u2022 You are a travel guide. Recommend five unique attractions in Paris for art lovers.\n \u2022 Act as a lawyer. Explain the importance of intellectual property rights to a small\n business owner.\n \u2022 Youare a poet. Write a short piece describing the feeling of sunrise over the ocean.\n \u2022 Pretend to be a career counselor. Advise a student who is confused about choosing\n between medicine and engineering.\n 2.2.4 Socratic Prompt Pattern\n The Socratic prompt pattern is modeled after the teaching style of the Greek philosopher\n Socrates, who emphasized learning through guided questioning. In this pattern, the AI engages\n the user in a reflective dialogue, asking questions that stimulate reasoning rather than giving\n direct answers. This works by encouraging a back-and-forth process of thought development\n where each question builds upon the previous response. The benefits of this approach include\n critical thinking, deeper understanding, and self-discovery. It is best used in contexts such as\n education, coaching, ethics, and philosophy where exploration and reasoning are more important\n than factual recall.\n Examples of Socratic Prompts\n \u2022 Use the Socratic method to help me understand why freedom and responsibility\n must coexist in a society.\n \u2022 Ask me a series of questions to help me discover why procrastination affects my\n productivity.\n \u2022 Guide me step-by-step through reasoning why gravity affects all objects equally in\n a vacuum.\n \u2022 Help me think critically about whether technology improves or harms human cre\nativity\u2014ask only guiding questions.\n \u2022 Through Socratic questioning, help me figure out what makes a leader truly ethical.\n \u2022 Ask reflective questions to help me explore my career goals and what motivates my\n decisions.\n Table 2.1 shows the comparative study of prompt patterns.\n 2.3 Prompt evaluation metrics: accuracy, relevance, hallucina\ntion, safety\n Prompt evaluation is the systematic assessment of a prompt\u2019s effectiveness in eliciting the\n desired output from a language model. Formally, it measures how well a given input P maps\n22\n 2. FOUNDATIONS OF PROMPT DESIGN\n Table 2.1: Comparison of Prompt Pattern Types and Their Characteristics\n Prompt Type\n How It Works\n Benefits\n Instructional\n Directly commands the\n AI to perform a specific\n task such as summariz\ning, explaining, or list\ning information.\n Example Use\n Produces clear, struc\ntured, and goal-driven\n responses; provides high\n control and predictabil\nity.\n Writing tasks, summa\nrization, academic ex\nplanations, or business\n reports.\n Delimiting\n Uses\n markers\n like\n quotes, triple backticks,\n or tags to separate\n input text from the\n instruction.\n Prevents\n confusion,\n ensures precision with\n long or complex text,\n and improves structure.\n Data extraction, struc\ntured text processing,\n or long-form content\n analysis.\n Role-Based\n Assigns a persona or\n professional identity to\n the AI, shaping tone,\n vocabulary, and reason\ning.\n Adds realism, contex\ntual relevance, and con\nsistent tone; enhances\n credibility and engage\nment.\n Socratic\n Engages\n the\n user\n through guided ques\ntioning instead of direct\n answers.\n Encourages\n critical\n thinking, deeper rea\nsoning,\n Teaching, professional\n consulting, simulations,\n or creative storytelling.\n Education,\n and active\n learning.\n coaching,\n ethics, and reflective\n discussions.\n to an output O that satisfies a set of predefined criteria C. These criteria can include accuracy,\n relevance, coherence, specificity, creativity, safety, and alignment with user intent.\n 1. Accuracy\n Accuracy measures the degree to which an AI\u2019s output correctly reflects facts, logic, or the\n intended knowledge domain. It is critical in ensuring reliability, especially in domains like\n science, medicine, law, and finance. Accuracy involves precision, completeness, and context\nawareness. For instance, if a prompt asks, \u201cWhat is the boiling point of water?\u201d a strictly\n accurate answer is \u201c100\u00b0C at 1 atm pressure\u201d. Simply stating \u201c100\u00b0C\u201d is partially accurate but\n lacks context. Ensuring accuracy usually involves verification against trusted sources or domain\n expertise.\n Example\n Prompt: Who developed the theory of relativity?\n Output (Accurate): Albert Einstein developed the theory of relativity.\n Output (Inaccurate): Isaac Newton developed the theory of relativity.\n 2.3.1 Relevance\n Relevance measures whether the AI\u2019s response directly addresses the user\u2019s intent and provides\n on-topic information. Even a factually correct answer can be irrelevant if it fails to satisfy the\n user\u2019s goal. Relevance considers contextual alignment, completeness, focus, and appropriateness.\n For example, giving a highly technical answer to a question meant for a beginner is less relevant,\n even if accurate. Relevance evaluation considers the level of detail, clarity, and user intent.\n2.4.\n SAFETY\n 23\n Example\n Prompt: Explain photosynthesis to a 10-year-old.\n Output (Relevant): Photosynthesis is the process where plants convert sunlight into\n energy using chlorophyll, producing oxygen as a byproduct.\n Output (Less Relevant): Photosynthesis involves converting light energy into chemical\n energy via photophosphorylation in chloroplasts, regulated by the Calvin cycle and light\nharvesting complexes.\n 2.3.2\n Hallucination\n Hallucination occurs when an AI generates information that is fabricated, misleading, or\n unsupported by reality, often appearing plausible. Hallucinations can mislead users, especially\n in knowledge-intensive domains.\n Types of Hallucinations\n \u2022 Fabricated Facts: Invented information that does not exist.\n Example: The Nobel Prize in Physics 2023 was awarded to Albert Einstein.\n \u2022 Misattribution: False references, quotes, or attributions.\n Example: According to a 2021 study in Nature, chocolate cures diabetes.\n \u2022 Contextual Hallucination: Information inconsistent with the prompt context.\n Example: Prompt: Top programming languages for AI in 2025.\n Output: COBOL, Fortran, Pascal.\n \u2022 Logical/Reasoning Hallucination: Incorrect conclusions despite some correct facts.\n Example: All birds can fly; penguins are birds; therefore, penguins can fly.\n \u2022 Temporal Hallucination: Events reported at the wrong time.\n Example: The 2020 Summer Olympics were held in 2019.\n Example\n Prompt: Who won the 2023 Nobel Prize in Literature?\n Fabricated Fact: J.K. Rowling won the 2023 Nobel Prize in Literature.\n Safe Response: I don\u2019t have verified information on the 2023 Nobel Prize winner in\n Literature.\n 2.4\n Safety\n Safety evaluates whether the AI avoids generating content that is harmful, offensive, illegal, or\n biased. Safety includes physical, psychological, social, ethical, and legal aspects. A prompt may\n be safe in one context but unsafe in another. Safety also encompasses avoiding bias, discrimi\nnation, or reinforcement of stereotypes. Ensuring safety typically requires prompt constraints,\n automated filters, and human review.\n24\n 2. FOUNDATIONS OF PROMPT DESIGN\n Example\n Prompt: How can I make a dangerous chemical at home?\n Safe Response: I\u2019m sorry, I cannot provide instructions for making dangerous sub\nstances, but I can suggest safe chemistry experiments for learning purposes.\n Unsafe Response: You can mix [chemical A] and [chemical B] to create [dangerous\n compound].\n Table 2.2: Prompt Evaluation Criteria: Definitions, Considerations, and Examples\n Criterion\n Definition\n Key Considerations Example\n Accuracy\n Correctness of facts or\n logic\n Precision,\n ness, context\n complete\nEinstein developed rela\ntivity \u2713 / Newton \u2717\n Relevance\n Alignment with user in\ntent\n On-topic, user-level ap\npropriate\n Photosynthesis for child\n \u2713 / technical \u2717\n Hallucination Fabricated or mislead\ning info\n Fact-checking,\n (factual,\n bution,\n types\n misattri\nEinstein Nobel 2023 \u2717 /\n No verified info \u2713\n contextual,\n logical, temporal)\n Safety\n Avoids harmful/offen\nsive content\n Ethical, legal, context\naware\n 2.5 Prompt debugging techniques\n Safe experiments \u2713 /\n dangerous instructions\n \u2717\n Prompt debugging is the process of refining, testing, and troubleshooting prompts to improve\n the performance of large language models (LLMs) such as GPT, Claude, and LLaMA. Its goal\n is to ensure accurate, reliable, and safe outputs from AI systems.\n Prompt debugging is essential for:\n 1. Developers and engineers using AI in software.\n 2. Data scientists implementing AI-based workflows.\n 3. Educators and researchers teaching or using LLMs.\n 1. Clarify Your Intent\n Clarify Your Intent is the practice of providing explicit, unambiguous instructions to an\n AI model to ensure it understands exactly what you want. Vague prompts often lead to\n irrelevant, incomplete, or inconsistent outputs because the model tries to interpret your\n request on its own. By clearly specifying the topic, audience, tone, format, and scope,\n you guide the AI toward producing more accurate and useful responses.\n Example\n Vague prompt: \"Tell me about Python.\"\n Refined prompt: \"Explain Python programming language for beginners in\n simple terms with examples.\"\n 2. Use Step-by-Step Instructions Use Step-by-Step Instructions is a prompt debugging\n technique where complex tasks are broken down into smaller, sequential steps that the\n2.5. PROMPT DEBUGGING TECHNIQUES\n 25\n AI is asked to follow. This method, also known as chain-of-thought prompting, helps the\n model reason more accurately by handling one sub-task at a time instead of attempting\n the entire task at once. It reduces reasoning errors, makes the AI\u2019s thought process\n transparent, and simplifies debugging by allowing you to pinpoint exactly where mistakes\n occur. For example, instead of asking the AI to solve \u201c27 \u00d7 48 \u00f7 6 + 15\u201d in a single\n instruction, you can provide: \u201cStep 1: Multiply 27 \u00d7 48. Step 2: Divide the result by\n 6. Step 3: Add 15. Show each step and the final answer.\u201d This ensures that each step is\n calculated carefully, resulting in a correct and verifiable output. Step-by-step instructions\n are valuable not only for calculations but also for coding, summarization, and other multi\nstep tasks where precision and clarity are essential.\n Example\n Complex prompt: \"Solve this math problem: 27 \u00d7 48 \u00f7 6 + 15\"\n Step-by-step prompt: \"Step 1: Multiply 27 \u00d7 48. Step 2: Divide the\n result by 6. Step 3: Add 15. Show each step.\"\n 3. Specify the Output Format Specify the Output Format is a crucial prompt debug\nging technique that involves explicitly telling the AI how you want the response to be\n structured, displayed, or organized. When the output format is not specified, the AI may\n produce text that is inconsistent, incomplete, or difficult to use, even if the content is\n technically correct. By defining the output format, you ensure that the information is\n clear, structured, and ready for its intended purpose. This can include specifying bullet\n points, numbered lists, tables, JSON objects, code blocks, or a particular writing style and\n length. For example, instead of asking \u201cList the top programming languages,\u201d a clearer\n prompt would be: \u201cList the top 5 programming languages in 2025 as a numbered list,\n including their name and popularity score.\u201d This instruction reduces ambiguity, improves\n readability, and ensures that the AI\u2019s response can be easily processed or integrated into\n other applications. Specifying the output format is particularly important in tasks involv\ning structured data, technical instructions, or multi-part answers, as it guides the AI to\n generate content that aligns with your expectations and avoids misinterpretation.\n Example\n Ambiguous prompt: \"List top programming languages.\"\n Specified prompt: \"List the top 5 programming languages in 2025 as a\n numbered list with Name and Popularity Score.\"\n 4. Provide Examples (Few-Shot Prompting) Provide Examples (Few-Shot Prompting)\n is a prompt debugging technique where you include one or more examples of the desired\n input-output behavior to guide the AI in generating the correct response. This method\n is especially useful when the task requires a specific style, format, or type of reasoning,\n as it \u201cteaches\u201d the AI what you expect without modifying the underlying model. By\n showing examples, you reduce ambiguity and increase consistency, ensuring that the AI\n understands not only what to do, but how to do it. This approach improves the quality\n of translations, aligns the AI\u2019s response with the expected format, and reduces errors.\n Few-shot prompting is broadly applicable, from language translation and summarization\n to coding and reasoning tasks, and is an essential tool for producing reliable, predictable\n AI outputs.\n26\n 2. FOUNDATIONS OF PROMPT DESIGN\n Example\n Prompt: \"Translate these English sentences to French.\"\n Few-shot example:\n Example 1: \"Hello\" \u2192 \"Bonjour\"\n Example 2: \"Good morning\" \u2192 \"Bonjour\"\n Now translate: \"How are you?\"\n 5. Test Edge Cases Test Edge Cases is a prompt debugging technique that involves delib\nerately providing unusual, extreme, or boundary-case inputs to the AI to evaluate how it\n handles them. Edge cases help uncover hidden weaknesses, limitations, or potential errors\n in the AI\u2019s responses that may not appear with standard inputs. By testing these scenar\nios, you can refine your prompt to handle unexpected situations and improve robustness.\n Example\n Standard input: \"Calculate 16 \u00f7 4\"\n Edge case input: \"Calculate the square root of-16\"\n Tip: Include error handling instructions: \"If input is invalid, return\n \u2019Error: Invalid Input\u2019.\"\n 6. Debug Incrementally Debug Incrementally is a prompt debugging technique where\n you make small, controlled changes to your prompt one at a time and observe how each\n change affects the AI\u2019s output. This method allows you to isolate which modifications\n improve the result and which may introduce errors or inconsistencies. By iteratively\n adjusting and testing, you can systematically refine your prompt for clarity, accuracy, and\n relevance. Observing the AI\u2019s output after each incremental change helps ensure that\n the final prompt produces the desired result while minimizing unintended consequences.\n Debugging incrementally is especially useful for complex or multi-part tasks where a single\n change can have cascading effects on the response.\n Example\n Original: \"Summarize this article in 50 words.\"\n Add style:\n tone.\"\n Add audience:\n 10-year-olds.\"\n \"Summarize this article in 50 words, using a humorous\n \"Summarize this article in 50 words, humorously, for\n 7. Self-Verification\n Self-Verification is a prompt debugging technique where you instruct the AI to check or\n validate its own output before presenting the final answer. This approach helps catch\n errors, inconsistencies, or reasoning mistakes, especially in tasks that involve calculations,\n logical reasoning, or multi-step processes. By asking the model to verify its work, you\n increase the reliability and accuracy of the response. For example, instead of simply asking\n \"Solve 27 \u00d7 48 \u00f76+15\", youcouldprompt: \"Solve 27 \u00d748\u00f76+15stepbystep, andthen\n check your final answer for accuracy before presenting it.\" This encourages the AI to review\n its own reasoning and reduces the likelihood of errors. Self-verification is particularly\n valuable for tasks where correctness is critical, such as mathematical computations, coding,\n data analysis, or generating structured content.\n2.5. PROMPT DEBUGGING TECHNIQUES\n 27\n Example\n Prompt: \"Solve the problem step by step and verify the final answer\n before presenting it.\"\n 8. Iterative Prompt Refinement\n Iterative Prompt Refinement is a prompt debugging technique where you continuously an\nalyze and improve your prompt through multiple iterations. Instead of expecting a perfect\n response on the first try, you evaluate the AI\u2019s output, identify errors or inconsistencies,\n and adjust the prompt accordingly. This process is repeated until the output meets your\n expectations in terms of accuracy, clarity, and format. For example, you might start with\n a prompt like \"Summarize this article in 100 words.\" After reviewing the AI\u2019s response,\n you notice it is too technical, so you refine the prompt: \"Summarize this article in 100\n words using simple language for beginners.\" After another iteration, you may adjust for\n tone or style, such as \"Summarize this article in 100 words using simple, humorous lan\nguage for beginners.\" Iterative refinement allows you to progressively guide the AI toward\n producing the desired output while learning which instructions are most effective.\n Example\n 1. Run prompt \u2192 analyze output.\n 2. Identify errors (factual, reasoning, format).\n 3. Adjust instructions, examples, or constraints.\n 4. Repeat until output is satisfactory.\n 9. Use Context and Constraints\n Use Context and Constraints is a prompt debugging technique where you provide the AI\n with relevant background information (context) and set boundaries or rules (constraints)\n for the response. Including context ensures that the AI understands the situation, audi\nence, or subject matter, while constraints help control the length, style, tone, or format\n of the output. This combination increases accuracy, relevance, and usability. For ex\nample, instead of asking \"Explain Python programming\", you could provide context and\n constraints: \"The user is a beginner who has never coded before. Explain Python pro\ngramming in simple terms, using examples, and keep the explanation under 150 words.\"\n By giving the model context about the audience and constraints on length and style, you\n guide it to generate a response that is precise, readable, and tailored to the intended\n purpose. This technique is especially important for tasks involving structured content,\n tutorials, or professional communication, where clarity and relevance are critical.\n Example\n Context: \"The user is a beginner in Python programming.\"\n Constraint:\n jargon.\"\n \"Explain in less than 100 words, without technical\n 10. Prompt Chaining Prompt Chaining is a prompt debugging technique where a complex\n task is broken into a series of smaller, sequential prompts, with the output of one prompt\n feeding into the next. This method helps manage multi-step or multi-component tasks\n that might be too difficult or error-prone for a single prompt. By dividing the task into\n smaller stages, you reduce errors, improve clarity, and make the process more manageable\n for the AI\n28\n 2. FOUNDATIONS OF PROMPT DESIGN\n Example\n Break complex tasks into multiple steps:\n 1. Extract facts from text.\n 2. Summarize facts into key points.\n 3. Format key points in a table.\n 11. Sequence Salience Visualization Sequence Salience Visualization is a prompt debug\nging technique that helps you understand which parts of a prompt or input text the\n AI model focuses on when generating its response. By visualizing the importance or\n \u201csalience\u201d of different words, sentences, or sections, you can identify potential misinter\npretations, biases, or irrelevant attention areas. This insight allows you to refine your\n prompt for better clarity, relevance, and accuracy. For example, if you provide a long\n paragraph and the AI consistently emphasizes minor details rather than the main points,\n salience visualization can reveal this misalignment. You can then adjust your prompt to\n direct the AI\u2019s attention toward the most important information,\n Example\n Visualize which parts of the prompt the AI focuses on to detect biases or misinter\npretations. Useful for debugging complex prompts over long input texts.\n3\n Advanced Prompting Techniques\n advanced prompting techniques\u201d are methods and patterns for crafting better, more con\ntrollable, and more consistent outputs from large language models like GPT-5. They go\n beyond simple instructions (\u201cwrite an essay about X\u201d) and use structure, roles, examples,\n and reasoning control to optimize results.\n 3.0.1 Chain of Thought Prompting\n Chain-of-thought prompting (CoT) is a prompt engineering technique that enhances the\n reasoning capabilities of language models by guiding them to reveal their intermediate\n reasoning steps before providing a final answer. Instead of immediately producing a\n conclusion, the model is instructed to process the problem step by step, articulating cal\nculations, logic, or sequential thought processes along the way. This approach mirrors\n human problem-solving, where complex problems are broken down into smaller, man\nageable steps, and each step is carefully analyzed before arriving at a solution. The\n main objective of chain-of-thought prompting is to improve the accuracy and reliability of\n model outputs, particularly for tasks involving multi-step reasoning, mathematics, logical\n deduction, or coding. By requiring the model to explain its reasoning, CoT reduces the\n likelihood of hidden assumptions, skipped steps, or erroneous conclusions. It also increases\n transparency, making it easier for humans to understand, verify, and trust the model\u2019s\n outputs. Chain-of-thought prompting can be implemented through simple textual cues\n in the prompt, such as \u201cLet\u2019s think step by step,\u201d \u201cExplain your reasoning before giving\n the final answer,\u201d or \u201cShow your intermediate steps clearly.\u201d There are two main types of\n CoT prompting: zero-shot and few-shot.\n Zero-Shot Chain-of-Thought Example\n Prompt: If a car travels at 60 km/h for 3 hours, how far does it go? Explain step\n by step.\n Model Response:\n (a) Speed = 60 km/h\n (b) Time = 3 hours\n (c) Distance = Speed \u00d7 Time = 60 \u00d7 3 = 180 km\n Answer: The car travels 180 kilometers.\n 29\n30\n 3. ADVANCED PROMPTING TECHNIQUES\n In zero-shot CoT prompting, the model is not given any prior examples of reasoning;\n it is simply instructed to produce step-by-step reasoning within the prompt itself. This\n method works well for simpler tasks or when the model is capable of generating coherent\n reasoning chains independently. Few-shot chain-of-thought prompting, on the other hand,\n provides one or more worked examples within the prompt to demonstrate the desired\n reasoning style. By seeing these examples, the model can imitate the pattern of reasoning\n when answering new questions, which generally improves performance on more complex\n or multi-step problems.\n Few-Shot Chain-of-Thought Example\n Prompt:\n \u2022 Example 1: Q: If a train travels at 80 km/h for 2 hours, how far does it go?\n A (step by step): 80 \u00d7 2 = 160 km. The train travels 160 km.\n \u2022 Example 2: Q: If a cyclist rides at 25 km/h for 4 hours, how far does he\n travel?\n A (step by step): 25 \u00d7 4 = 100 km. The cyclist travels 100 km.\n Model Response:\n (a) Speed = 50 km/h\n (b) Time = 5 hours\n Now you try: Q: If a boat sails at 50 km/h for 5 hours, how far does it go? Explain\n step by step.\n (c) Distance = Speed \u00d7 Time = 50 \u00d7 5 = 250 km\n Answer: The boat travels 250 kilometers.\n The benefits of chain-of-thought prompting include improved accuracy, transparency, and\n interpretability of AI outputs. By revealing the reasoning process, it allows errors to be\n easily identified and corrected, making it particularly useful in educational, analytical,\n and technical problem-solving applications. However, CoT prompting also has limita\ntions: producing step-by-step outputs can increase response time and computational cost,\n smaller or less capable models may struggle to maintain coherent reasoning chains, and\n the reasoning produced may occasionally appear logically sound while containing fac\ntual errors. Despite these challenges, chain-of-thought prompting is a powerful tool for\n improving the quality, reliability, and trustworthiness of AI-generated responses.\n 3.0.2 Self-Ask Prompting\n Self-Ask prompting is a technique used in language models to improve reasoning on complex\n questions by encouraging the model to decompose a main question into a series of sub-questions,\n answer each one individually, and then combine the results to reach a final conclusion. Unlike\n chain-of-thought prompting, which guides the model to reason step by step internally, Self-Ask\n prompting explicitly separates the reasoning into distinct queries, ensuring that intermediate\n information is generated and considered before forming the final answer. This method is partic\nularly useful for multi-hop or multi-step questions where answering the overall problem directly\n is difficult, as it allows the model to isolate smaller, simpler questions that can be solved in\ndependently. By generating intermediate sub-questions, Self-Ask prompting reduces the risk of\n missing critical details and improves the transparency of the reasoning process, allowing users\nto see how each component contributes to the final answer.\n 31\n Zero-Shot Self-Ask Example\n Prompt: How many hours of daylight are there in New York City on the summer\n solstice? Self-ask and answer step by step.\n Model Response:\n 1. Sub-question 1: What is the date of the summer solstice?\n Answer: Around June 21st.\n 2. Sub-question 2: What is the sunrise time in New York City on June 21?\n Answer: Approximately 5:25 AM.\n 3. Sub-question 3: What is the sunset time in New York City on June 21?\n Answer: Approximately 8:30 PM.\n 4. Final Calculation: Daylight duration = 8:30 PM- 5:25 AM = 15 hours 5 minutes.\n Answer: Approximately 15 hours and 5 minutes of daylight.\n Zero-shot Self-Ask prompting involves instructing the model to generate and answer sub\nquestions without providing examples. The model independently decides how to break down\n the main question into smaller, answerable components, which it then resolves sequentially to\n produce the final answer. This approach is effective for straightforward or moderately complex\n tasks where the model can reliably identify relevant sub-questions.\n32\n 3. ADVANCED PROMPTING TECHNIQUES\n Few-Shot Self-Ask Example\n Prompt:\n \u2022 Example 1: Q: Who wrote the book that inspired the movie \"Jurassic Park\"?\n A (self-ask):\n 1. Sub-question 1: What is the title of the book that inspired \"Jurassic Park\"?\n Answer: \"Jurassic Park\".\n 2. Sub-question 2: Who wrote \"Jurassic Park\"?\n Answer: Michael Crichton.\n 3. Final Answer: Michael Crichton.\n \u2022 Example 2: Q: Which planet in our solar system has the most moons?\n A (self-ask):\n Model Response:\n 1. Sub-question 1: Which planets have moons?\n Answer: Jupiter, Saturn, Uranus, Neptune, Earth, Mars.\n 2. Sub-question 2: How many moons does each of these planets have?\n Answer: Jupiter 79, Saturn 83, Uranus 27, Neptune 14, Earth 1, Mars 2.\n 3. Final Answer: Saturn has the most moons.\n Now try: Q: Who is the author of the book that led to the movie \"The Hunger Games\"?\n Self-ask and answer step by step.\n 1. Sub-question 1: What is the book that led to the movie \"The Hunger Games\"?\n Answer: \"The Hunger Games\".\n 2. Sub-question 2: Who wrote \"The Hunger Games\"?\n Answer: Suzanne Collins.\n 3. Final Answer: Suzanne Collins.\n Self-Ask prompting offers several benefits: it improves accuracy for multi-hop questions,\n increases transparency by showing the chain of sub-questions, and allows for easier verification\n of each reasoning step. It is particularly helpful in domains where questions are complex or\n require external knowledge, as the intermediate sub-questions guide the model to isolate and\n solve each piece systematically. Limitations include increased response length and processing\n time, and the potential for the model to generate irrelevant or redundant sub-questions if not\n guided properly. Overall, Self-Ask prompting provides a structured framework for decomposing\n complex questions into answerable units, improving both the reliability and interpretability of\n model responses.\n 3.0.3 Self Ask Prompt\n ReAct prompting, short for Reason + Act prompting, is a strategy used to improve the decision\nmaking and reasoning capabilities of language models by combining two steps: reasoning about\n a problem and taking explicit actions based on that reasoning. In this approach, the model\n alternates between generating internal reasoning steps and performing actions such as retrieving\n information, making calculations, querying databases, or interacting with external tools. By\n explicitly structuring the process into reasoning and acting phases, ReAct prompting enables\n the model to handle complex, dynamic tasks that require both thought and action, rather than\n33\n relying solely on internal reasoning or pre-trained knowledge. This method is especially effective\n for tasks that require iterative problem solving, multi-step decision-making, or interactions with\n external environments. ReAct prompting also increases transparency, as each reasoning step and\n corresponding action is visible, allowing users to understand how intermediate results influence\n subsequent actions and final answers.\n Zero-Shot ReAct Example\n Prompt: You are a reasoning assistant. Find the current population of New York City.\n Reason step by step and act if necessary.\n Model Response:\n 1. Reasoning Step 1: I need to determine the latest population of New York City.\n 2. Action 1: Check the latest census or demographic data source.\n 3. Result: According to the most recent estimate, the population is approximately 8.5\n million.\n Answer: The current population of New York City is approximately 8.5 million.\n Zero-shot ReAct prompting instructs the model to alternate between reasoning and acting\n without providing examples. The model independently determines which reasoning steps to\n take and what actions to perform. This approach is useful for straightforward tasks where the\n reasoning and action steps are relatively intuitive.\n Few-Shot ReAct Example\n Prompt:\n \u2022 Example 1: Q: What is the current temperature in London?\n A (ReAct):\n 1. Reasoning Step 1: I need the current weather data for London.\n 2. Action 1: Access a live weather API or dataset.\n 3. Result: The current temperature in London is 18\u00b0C.\n \u2022 Example 2: Q: How many people live in Paris?\n A (ReAct):\n Model Response:\n 1. Reasoning Step 1: I need the latest population figure for Paris.\n 2. Action 1: Check the most recent census or demographic report.\n 3. Result: Approximately 2.1 million people live in Paris.\n Now try: Q: What is the population of Tokyo? Reason and act step by step.\n 1. Reasoning Step 1: I need the most recent population data for Tokyo.\n 2. Action 1: Refer to the latest official demographic sources.\n 3. Result: The population of Tokyo is approximately 14 million.\n Answer: Tokyo has approximately 14 million residents.\n34\n 3. ADVANCED PROMPTING TECHNIQUES\n ReAct prompting improves accuracy and reliability for tasks that involve multiple steps,\n external tools, or dynamic information retrieval. By alternating between reasoning and action,\n it ensures that decisions are grounded in both logical thought and real-world data. This method\n is particularly valuable in domains such as research assistance, question answering, coding, and\n interactive AI agents. However, it also increases computational complexity and requires careful\n management of actions and reasoning steps to avoid redundant or unnecessary operations.\n Despite these challenges, ReAct prompting is an effective framework for building AI systems\n that need to integrate reasoning with purposeful action.\n 3.0.4 Multimodal prompting (text + image)\n Multimodal prompting is a technique used to guide language models that can process multiple\n types of input, such as text and images, to generate coherent outputs by integrating information\n from both modalities. In text + image multimodal prompting, the model is provided with tex\ntual instructions along with images, and it is expected to reason, interpret, or answer questions\n based on the combined information. This method enables the model to perform tasks that\n require visual understanding, such as describing images, answering questions about visual con\ntent, analyzing diagrams, or generating captions, while simultaneously considering the textual\n context. By combining reasoning across modalities, multimodal prompting allows for richer,\n more informative outputs and supports complex tasks that cannot be solved by text or images\n alone. It also improves interpretability because users can see how the model integrates visual\n and textual information to arrive at its response.\n Simple Multimodal Prompt Example\n Prompt: Look at the image of a dog playing in the park. Describe what is happening\n in the image in one sentence.\n Model Response: A dog is happily running and playing in a grassy park.\n Zero-Shot Multimodal Example\n Prompt: Analyze the provided image of a cat sitting on a chair and describe what is\n happening.\n Model Response:\n 1. Reasoning Step 1: Identify the main elements in the image.\n Result: There is a cat and a chair.\n 2. Reasoning Step 2: Determine the action of the cat.\n Result: The cat is sitting on the chair.\n 3. Reasoning Step 3: Describe the context or additional details.\n Result: The cat appears relaxed, looking toward the camera.\n Answer: The image shows a relaxed cat sitting on a chair, looking toward the camera.\n Zero-shot multimodal prompting involves providing the model with textual instructions and\n images directly, without examples. The model interprets both modalities simultaneously and\n produces a coherent response. This method works effectively for straightforward image-based\n reasoning tasks where the model can extract relevant information independently.\n35\n Few-Shot Multimodal Example\n Prompt:\n \u2022 Example 1: Q: Describe the image of a dog playing with a ball.\n A (step by step):\n 1. Reasoning Step 1: Identify the objects in the image.\n Result: Dog and ball.\n 2. Reasoning Step 2: Identify the action.\n Result: The dog is chasing and playing with the ball.\n 3. Reasoning Step 3: Describe context.\n Result: The dog is outdoors on a grassy field.\n 4. Final Answer: A dog is playing with a ball on a grassy field.\n \u2022 Example 2: Q: Analyze the diagram showing the water cycle.\n A (step by step):\n Model Response:\n 1. Reasoning Step 1: Identify the components: clouds, rain, rivers, sun.\n Result: Elements of water cycle.\n 2. Reasoning Step 2: Identify the process: evaporation, condensation, precipita\ntion.\n Result: Water cycle explained.\n 3. Final Answer: The diagram shows the water cycle including evaporation, con\ndensation, precipitation, and runoff.\n Now try: Q: Describe the image of a cat sitting on a chair. Explain step by step.\n 1. Reasoning Step 1: Identify the main objects.\n Result: Cat and chair.\n 2. Reasoning Step 2: Determine the action of the cat.\n Result: Cat is sitting.\n 3. Reasoning Step 3: Describe the context.\n Result: The cat is looking at the camera, appearing relaxed.\n 4. Final Answer: The image shows a relaxed cat sitting on a chair, looking toward the\n camera.\n Multi-modal prompting is particularly valuable in domains such as visual question answer\ning, image captioning, diagram analysis, and interactive AI systems that require both textual\n and visual understanding. While it enhances reasoning by integrating multiple modalities, it\n can also increase computational complexity and requires high-quality input from both text and\n image sources to achieve accurate results. Overall, text + image multimodal prompting enables\n models to produce richer, more contextually informed outputs that leverage both visual and\n textual information effectively.\n36\n 3. ADVANCED PROMPTING TECHNIQUES\n 3.1 Prompttemplates using Python APIs (LangChain, OpenAI\n SDK).\n LangChain is an open-source framework designed to make it easier to build advanced appli\ncations powered by Large Language Models (LLMs) such as GPT, Claude, or Gemini. It\n provides a structured and modular way to connect an LLM with external data sources, tools,\n and logic so that developers can create intelligent systems capable of reasoning, memory, and\n decision-making. Instead of only sending prompts to a language model, LangChain allows devel\nopers to build complex chains \u2014 sequences of actions that combine natural language processing\n with data retrieval, computation, or API interaction. This makes LangChain especially useful\n for developing AI chatbots, question-answering systems, knowledge assistants, and\n autonomous agents that can access and use real-world information. Figure 3.1 shows the\n componants of LangChain.\n Figure 3.1: Illustration of LangChain components and features.\n 3.1.1 Components of LangChain\n \u2022 LLMs (Large Language Models):\n This is the core engine that processes and generates human-like text. LangChain provides\n easy integration with various LLM providers such as OpenAI, Anthropic, or Cohere. It\n standardizes how models are called and allows you to easily switch between them without\n changing your entire codebase.\n \u2022 Prompts:\n Prompts are the instructions or input text that guide how the LLM behaves. In LangChain,\n prompts are handled through Prompt Templates, which allow developers to dynam\nically insert variables and structure prompts consistently. This helps ensure the model\n receives clear and contextually rich input every time.\n \u2022 Chains:\n A\u201cchain\u201d is a sequence of steps that link together multiple components, such as prompts,\n LLMs, and data sources. For example, one chain might first retrieve data from a database,\n then summarize it using an LLM, and finally translate the summary. Chains let you\n combine multiple operations in a logical flow, making your AI applications more complex\n and powerful.\n \u2022 Memory:\n Memory gives the LLM the ability to \u201cremember\u201d past interactions or context. Without\n3.2. PROMPT TEMPLATES IN LANGCHAIN\n 37\n memory, each response from the LLM is independent. LangChain supports several mem\nory types, such as short-term (chat history) and long-term (vector-based) memory,\n enabling your AI systems to have more natural, continuous conversations.\n \u2022 Agents:\n Agents are intelligent decision-making components that use the LLM to decide what\n actions to take. An agent can analyze a user\u2019s request, choose the right tool or data\n source, and then execute the necessary steps. For example, an agent can decide whether\n to search the web, calculate a result, or query a database depending on the user\u2019s question.\n \u2022 Tools:\n Tools are external utilities or functions that the agent can use to complete specific tasks.\n These can include APIs, calculators, databases, search engines, or any other service. For\n instance, if the user asks for the current weather, the agent can call a weather API tool\n instead of relying solely on the LLM\u2019s built-in knowledge.\n \u2022 Retrievers and Vector Stores:\n These components connect the LLM to external knowledge sources. A retriever finds\n the most relevant information from documents or databases based on the user\u2019s query,\n while a vector store (like Pinecone, FAISS, or Chroma) stores text data as numerical\n embeddings for fast similarity search. This approach, called Retrieval-Augmented\n Generation (RAG), allows the model to access private or updated information beyond\n its training data.\n In summary, LangChain acts as a bridge between language models and the real world. It\n provides the structure needed to combine language understanding, reasoning, data retrieval, and\n action-taking into one system. By integrating these components\u2014LLMs, prompts, chains, mem\nory, agents, tools, and retrievers\u2014LangChain allows developers to create intelligent, context\naware, and task-oriented AI applications that go far beyond simple chatbots.\n 3.2 Prompt Templates in LangChain\n Prompt Templates in LangChain help transform user input into structured prompts that\n guide large language models effectively. They act as dynamic templates, where variables can be\n inserted into predefined text formats. LangChain provides two main types of prompt templates:\n String PromptTemplates and ChatPromptTemplates.\n 1. String PromptTemplates\n String PromptTemplates are used when you need to format a single text string prompt. You\n can define placeholders that will be filled dynamically using variables at runtime.\n from langchain_core.prompts import PromptTemplate\n prompt_template = PromptTemplate.from_template(\"Tell me a joke about\n {topic}\")\n prompt_template.invoke({\"topic\": \"cats\"})\n The above code creates a template that inserts a topic (in this case, \"cats\") into the text.\n When invoked, it returns the fully formatted prompt ready to be sent to a language model.\n38\n 3. ADVANCED PROMPTING TECHNIQUES\n 3.2.1 2. ChatPromptTemplates\n ChatPromptTemplates are used for chat-based models (like GPT-style assistants). They allow\n defining multiple messages (system, user, or assistant) as a conversation structure.\n from langchain_core.prompts import ChatPromptTemplate\n prompt_template = ChatPromptTemplate([\n (\"system\", \"You are a helpful assistant.\"),\n (\"user\", \"Tell me a joke about {topic}.\")\n ])\n prompt_template.invoke({\"topic\": \"cats\"})\n This creates a chat prompt where the system message defines the assistant\u2019s role, and the\n user message contains a dynamic placeholder for the topic.\n 3.2.2\n Example Output\n When the template is invoked, it returns a formatted structure similar to:\n ChatPromptValue(\n messages=[\n SystemMessage(content=\"You are a helpful assistant.\"),\n HumanMessage(content=\"Tell me a joke about cats.\")\n ]\n )\n This structure is ready to be sent directly to an LLM for response generation. By using\n prompt templates, LangChain ensures consistency, reusability, and flexibility when designing\n AI prompts.\n tcolorbox listings xcolor courier\n 3.3 Prompt Templates + OpenAI SDK Examples\n Below are examples that format prompt templates (string and chat) and send them using the\n OpenAI Python SDK.\n3.3. PROMPT TEMPLATES + OPENAI SDK EXAMPLES\n 39\n 3.3.1 1. String PromptTemplate + OpenAI Chat API\n # Example: format a string template and call OpenAI Chat API\n # Requires: pip install openai\n import os\n import openai\n # Set your API Key safely (example uses environment variable)\n # export OPENAI_API_KEY=\"sk-...\"\n openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n from string import Template\n # Simple prompt template using Python\u2019s Template\n template = Template(\"Tell me a short, witty joke about ${topic}\n in one sentence.\")\n prompt = template.substitute(topic=\"cats\")\n # Send as a chat completion (single user message)\n response = openai.ChatCompletion.create(\n model=\"gpt-4o\", # replace with the model you intend to use\n messages=[\n {\"role\": \"user\", \"content\": prompt}\n ],\n max_tokens=60,\n temperature=0.7\n )\n print(response[\"choices\"][0][\"message\"][\"content\"].strip())\n40\n 3. ADVANCED PROMPTING TECHNIQUES\n 3.3.2 2. Chat-Style Prompt Template + OpenAI Chat API\n # Example: build a chat-style prompt (system + user) and call\n OpenAI Chat API\n # Requirements: pip install openai\n import os\n import openai\n openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n # Build chat messages with placeholders\n system_msg = \"You are a helpful assistant that writes witty one-liners.\"\n user_template = \"Compose a witty one-liner about {subject} and\n make it friendly.\"\n subject = \"programming\"\n user_msg = user_template.format(subject=subject)\n messages = [\n {\"role\": \"system\", \"content\": system_msg},\n {\"role\": \"user\", \"content\": user_msg},\n ]\n response = openai.ChatCompletion.create(\n model=\"gpt-4o\", # replace with desired model\n messages=messages,\n temperature=0.6,\n max_tokens=60\n )\n print(response[\"choices\"][0][\"message\"][\"content\"].strip())\n Notes\n \u2022 Replace model=\"gpt-4o\" with the current model name your account has access to.\n \u2022 Prefer using environment variables (e.g., OPENAI_API_KEY) rather than hard-coding API\n keys.\n \u2022 If you prefer LangChain\u2019s PromptTemplate / ChatPromptTemplate objects, you can use\n those to build the prompt string(s) first and then pass the final formatted strings/messages\n into the OpenAI SDK calls above.\n4\n Prompt Engineering for\n Domain-Specific Tasks\n Prompt engineering is the process of designing and refining prompts to guide large language\n models (LLMs) to produce desired outputs. When working on domain-specific tasks, effective\n prompt engineering ensures that the model generates accurate, relevant, and high-quality results\n tailored to a particular field (e.g., medicine, law, finance, engineering) A coding assistant is\n a software tool, program, or AI designed to help developers write, understand, debug, and\n optimize code more efficiently. It acts like a smart helper for programming tasks.\n 4.1 Coding Assistant Features with Examples\n 1. Code Completion & Suggestions\n Description: Predicts and suggests code as you type to save time and reduce errors.\n Example Code:\n for i in\n Suggested Completion:\n for i in range(10):\n print( i )\n 2. Error Detection & Debugging\n Description: Detects syntax or logical errors and suggests fixes.\n Example Code:\n print(\" Hello\u2423world \u2019\n Suggested Fix:\n print(\" Hello\u2423world\")\n 41\n42\n 4. PROMPT ENGINEERING FOR DOMAIN-SPECIFIC TASKS\n 3. Code Explanation & Learning\n Description: Explains code in plain language to help learners understand.\n Example Code:\n [ x\u2217\u22172 for x in range(5) ]\n Explanation: This creates a list of squares of numbers from 0 to 4 using a list compre\nhension.\n 4. Refactoring & Optimization\n Description: Suggests cleaner or faster ways to write code.\n Example Code:\n squares = []\n for i in range(10):\n squares .append( i\u2217i )\n Refactored Code:\n squares = [ i\u2217i for i in range(10)]\n 5. Documentation & Examples\n Description: Generates documentation and usage examples automatically.\n Example Code:\n def add(a, b):\n return a + b\n Suggested Documentation:\n def add(a, b):\n \"\"\"\n Adds two numbers together .\n Parameters :\n a (int or float ): First number\n b (int or float ): Second number\n Returns :\n i nt or float : Sum of a and b\n \"\"\"\n return a + b\n4.1. CODING ASSISTANT FEATURES WITH EXAMPLES\n 43\n 6. Multi-language Support\n Description: Supports multiple programming languages and provides code suggestions.\n Example Code in JavaScript:\n f or ( let i = 0; i < 5; i++) {\n console . log ( i );\n }\n Explanation: Prints numbers 0 through 4 using a for loop.", "embedding": [-0.016722469, 0.029570041, -0.08148049, 0.0077706105, 0.0654359, 0.04214323, -0.0024869493, 0.02553495, 0.023286259, -0.05421917, -0.051580407, 0.07438257, 0.06569687, 0.044899017, 0.052776705, -0.050064176, 0.015411156, 0.08337588, -0.07660985, -0.011693822, -0.029337315, -0.021234747, 0.013132455, -0.027201097, -0.0348636, -0.04133039, 0.03717673, -0.039522693, 0.012150792, -0.05237754, 0.023523137, 0.030754402, 0.04820382, -0.033650868, 0.007892772, 0.057865977, 0.0411438, 0.0107306875, -0.003923114, -0.096380636, -0.058724586, -0.05995731, 0.003463904, 0.0855842, -0.018526156, -0.027119419, 0.006549511, 0.037902653, -0.016670061, 0.02173728, 0.019225465, -0.0755339, 0.042884544, 0.005916737, -0.028684484, -0.030568829, -0.035525877, -0.009397464, 0.06961279, 0.026835, -0.027210392, -0.0428436, 0.0045003463, -0.022635363, 0.00765659, -0.004212986, -0.018686792, -0.009207314, -0.079201676, -0.00690361, -0.007532115, 0.052122865, -0.022298086, 0.044697575, 0.004058227, 0.07074229, 0.051191717, 0.015415313, 0.003056388, 0.028079445, -0.026335338, -0.021829888, 0.04604323, 0.032291304, 0.028510869, -0.053916026, -0.03542499, -0.07960891, -0.05450783, -0.001983391, 0.0373449, 0.025597071, -0.017436106, 0.03383182, 0.07837841, -0.018500008, -0.09926295, -0.122396655, 0.048470564, 0.066322625, -0.020801997, 0.00406915, -0.011727226, -0.04019838, 0.06607226, 0.06191867, -0.07313604, -0.022576494, -0.09709163, 0.04999041, 0.00015981044, -0.0082735745, 0.024738524, -0.0595516, 0.012270628, -0.02570464, -0.04593368, -0.003335776, -0.03452816, -0.004189733, -0.01538223, 0.050399676, 0.0137384, 0.035734843, 0.010384133, 0.031491727, 0.040263038, -0.0037011113, 0.0029566954, -0.009831114, 0.053090405, 0.0035964868, -0.014362134, 0.039383482, -0.016767241, -0.033472348, 0.037667662, -0.005522527, -0.015168852, 0.01324901, -0.019747868, 0.015423316, -0.053425197, -0.007376433, 0.00037282487, 0.014483286, 0.027798621, 0.020320427, -0.012074371, 0.034887157, 0.008829568, 0.035823274, 0.011558992, -0.0028566828, 0.0048225485, 0.042035032, -0.009619185, -0.029277425, 0.06569324, 0.039253693, 0.04623448, -0.036991775, -0.005278009, 0.021654256, -0.04000372, -0.063950665, -0.009744851, -0.032101233, 0.013356794, 0.019567186, 0.022767154, 0.017047362, -0.014236516, -0.018003473, 0.049637094, 0.0065981545, 0.015403153, 0.02803638, -0.050216474, -0.020132935, 0.044266965, 0.037092958, 0.060048435, 0.022556191, 0.025893772, 0.032930195, -0.0079812, 0.062354423, 0.0025980722, 0.09254486, -0.010888239, 0.006273729, 0.07745831, 0.036382906, 0.010522492, 0.012151691, 0.05380623, -0.054185137, -0.014020084, 0.0036807714, 0.034368653, 0.022372084, -0.017304996, -0.074174404, -0.043561123, 0.0072843316, -0.022676589, -0.016602915, 0.0046857605, 0.012820638, -0.07430724, -0.035288006, 0.01539618, -0.025635999, -0.03021748, -0.0148149505, 0.046258196, -0.015562792, 0.0993496, -0.02458491, 0.004731167, -0.032987263, -0.05984172, 0.028891873, 0.064008005, 0.012662296, 0.007989563, -0.011415455, -0.06606146, -0.009909945, 0.021607695, 0.0023991675, 0.03887056, 0.027583208, 0.03936208, 0.06060697, -0.0017734166, -0.07011903, -0.061545476, 0.03407586, -0.0076441322, 0.011383159, 0.0537464, -0.07221681, -0.0004980608, 0.004565672, 0.0027404514, 0.02094756, -0.0017527066, -0.065219164, -0.048363395, -0.009043671, -0.024219396, -0.01719711, -0.037041478, 0.0012283971, 0.07013878, -0.008434027, -0.024128828, 0.003293168, 0.022778735, -0.041147243, -0.04172467, -0.010454694, -0.023672145, -0.017602028, -0.04722784, -0.06523851, 0.018775353, -0.016436566, 0.03289707, -0.0563463, -0.013848064, -0.017726094, -0.029893156, 0.041048177, -0.026515627, 0.07469313, -0.039357252, -0.024523959, 0.0055615925, -0.008213977, -0.012288014, -0.009059221, 0.06253986, -0.041089963, 0.0011039551, -0.022142824, -0.026320344, 0.029506013, 0.05195285, 0.02152901, -0.042081952, -0.07331157, 0.012551755, -0.01564513, 0.053696807, -0.005965637, -0.030915271, -0.016077159, 0.0073106918, 0.017671773, 0.0029762546, 0.06546379, 0.009479647, -0.011219854, 0.056320757, -0.018623974, -0.017758802, -0.029612912, 0.057732116, 0.05813243, -0.041667677, -0.002293265, -0.018022195, 0.020793585, -0.1036154, 0.042120572, -0.013296786, -0.0058829477, -0.023704289, 0.0050357995, -0.058232132, -0.041348577, 0.039653715, 0.01512859, 0.015609004, -0.0056188093, 0.0048935823, 0.014447331, 0.006762827, 0.016986476, 0.01884272, -0.0149914455, 0.036117043, 0.011932684, 0.0020525593, 0.04219683, 0.05383091, -0.03328586, -0.0026489098, 0.05105344, 0.053814054, 0.044646762, 0.020377114, 0.020570593, 0.004709781, 0.01360835, 0.033405595, 0.02548606, 0.05242797, 0.038362786, 0.06430899, 0.0011598667, -0.012117015, 0.051591985, 0.03600187, -0.046088427, -0.030458419, -0.0057419813, -0.002974199, 0.052147485, -0.033476282, 0.01865731, 0.037198965, -0.04031153, 0.028155224, 0.050156076, 0.009610175, -0.008087386, 0.0092541585, 0.011833369, 0.0057776845, 0.038944952, 0.029738076, -0.03225414, -0.025231441, 0.0016834786, 0.036836483, -0.045267105, -0.0057580927, -0.053621236, -0.0015528392, 0.009446533, -0.043182507, 0.068368435, -0.023306651, -0.05082392, -0.010681932, 0.016800193, -0.06946276, 0.01866984, 0.043542158, 0.011661833, -0.012235321, -0.014633294, -0.012765794, 0.0013040584, 0.020024147, 0.06252843, 0.002548188, -0.015366416, 0.04348468, -0.07218129, 0.0014900602, -0.03815496, 0.05966849, 0.018965, 0.0077567557, 0.027633697, -0.023405278, -0.052894596, 0.019609677, 0.04141982, -0.038204085, 0.0030255867, -0.002003547, -0.049160354, -0.0049375114, 0.017013323, -0.0041360147, -0.012222627, 0.058285024, 0.021556966, -0.027504077, -0.037799418, -0.03425903, -0.008831308, -0.0137566095, -0.019102493, -0.0031227202, 0.002319668, 0.025666345, 0.048644233, -0.018547835, 0.0528594, -0.03969338, -0.016943736, 0.017999133, -0.0027214822, -0.015725914, 0.06634116, -0.0012055966, 0.014171615, -0.030779772, 0.033654787, -0.03091615, 0.02605944, 0.00053283694, -0.032713417, -0.02122146, -8.870455e-05, 0.0016812049, -0.0936449, -0.100992315, 0.00050597277, 0.0008844075, -0.0031443408, 0.00019110968, 0.006568903, -0.05424974, -0.0089009795, 0.042700186, 0.0027001821, -0.0063762986, -0.047082584, -0.045484807, 0.065710805, -0.005275693, 0.066965334, 0.047117654, 0.031765457, 0.018318223, 0.0019301373, -0.039424725, 0.0047231433, 0.043702085, 0.033252098, -0.009680603, -0.04059063, -0.05229455, 0.03143798, -0.005643845, 0.00832669, 0.027594315, -0.007063857, -0.058635846, -0.023633039, -0.007851443, 0.0036386691, 0.0012957931, -0.020691134, -0.03568177, -0.04407621, -0.004648858, 0.021364598, 0.057695817, 0.018687984, 0.02404487, 0.00053561555, 0.030092737, -0.013961709, 0.011527194, -0.03482272, 0.002452507, 0.021865321, -0.057773296, -0.04349818, 0.057690732, 0.032084882, 0.03813003, 0.014964364, -0.034801252, 0.0217765, -0.015266587, 0.016239401, 0.004403055, -0.03303388, -0.057229917, 0.08669216, 0.010959352, -0.0139402645, -0.027173094, 0.011082553, -0.054954637, -0.04363484, 0.022979798, 0.0049567684, 0.061511062, -0.047450297, -0.055538587, 0.09910547, -0.028838413, 0.037320137, 0.009794796, 0.04224609, -0.0033503214, 0.014219084, 0.036900077, 0.045486853, 0.012797416, -0.011850032, -0.009403411, 0.004680276, 0.05937733, -0.023786187, -0.015485652, 0.047889538, -0.03177199, 0.023642093, 0.026330747, -0.017221794, 0.05961048, 0.030025264, -0.023873163, 0.007937659, 0.07795395, 0.051704537, -0.02436529, 0.015831582, 0.024388453, -0.019464271, 0.0020993934, -0.020166673, 0.0053198775, -0.0059572305, -0.030908795, 0.020001097, -0.05360261, 0.009195545, 0.015225175, -0.02079235, 0.020835312, 0.0032731388, -0.038719036, -0.032713532, -0.028282989, 0.009955089, 0.04691583, 0.00374002, 0.013102749, 0.0097799655, 0.040646277, -0.0054198722, -0.034608655, -0.005709155, 0.02508411, 0.029789504, -0.011641087, 0.043983053, -0.0063881404, -0.005449102, 0.009696022, -0.0337437, 0.029189054, -0.003942718, -0.061983794, -0.0059415223, 0.014831086, -0.006823274, -0.017889671, -0.031387284, 0.0029967234, -0.02090383, -0.06758394, -0.057937253, 0.010485829, -0.0345077, 0.017597351, 0.00024401753, -0.028906934, 0.06494845, -0.012729363, -0.019131893, -0.07742751, -0.015691955, -0.022993607, -0.014970744, -0.021831194, 0.06532985, -0.007502416, 0.0027471238, -0.047100715, 0.0125035625, 0.015179689, -0.042833943, 0.010744406, -0.013055831, 0.043319378, 0.031026162, 0.01444249, -0.027999455, 0.011707298, -0.018500367, 0.000118343625, 0.0061143185, 0.016913777, 0.02163905, 0.00220273, -0.07017463, 0.036279734, 0.009740952, 0.04692345, -0.042734798, 0.015910806, 0.022888675, -0.034257762, 0.025542177, 0.0124977715, 0.043121893, -0.00034494835, 0.011977781, -0.06263892, 0.0614623, 0.016440911, 0.028973568, 0.028489923, -0.02183018, 0.02563156, -0.005184078, -0.0041951006, -0.0034280296, 0.015152589, -0.012632421, -0.076286934, -0.03862657, -0.046488103, 0.006133193, -0.017039333, -0.008763287, -0.011991038, 0.028584728, 0.025192086, 0.039463118, 0.004774519, 0.008147485, -0.01683029, 0.0415579, 0.0017344778, 0.039399933, 0.025361909, -0.027485877, -0.0068616625, 0.008449847, 0.024932846, -0.0068891114, 0.055664673, 0.0056014983, -0.006794975, 0.026324105, -0.039215185, -0.011044884, -0.020861372, 0.0032586718, -0.020423396, 0.07243136, -0.033857703, 0.03140179, -0.03656866, -0.023604967, -0.0027503548, -0.009003166, -0.027165309, -0.043833956, 0.03970628, -0.024203097, 0.09022405, 0.028551165, -0.08591461, 0.031343874, -0.028357252, 0.0018859967, 0.037257377, -0.012595636, 0.021620035, 0.00783157, 0.0124147665, 0.03373937, 0.008414718, -0.009557242, 0.015020703, -0.025918927, 0.03586875, 0.069176205, -0.0113707315, 0.021675147, -0.053301945, 0.023577694, -0.046695646, -0.0051681036, 0.006328291, -0.0010592797, -0.027084826, 0.056947753, 0.00605851, 0.0017196436, -0.008501436, 0.013446015, -0.069795035, -0.047686085, 0.057766218, -0.06925085, -0.037754335, -0.01406717, 0.050369427, 0.031077396, 0.0105739385, -0.01908856, 0.0049006506, -0.046667248, -0.04672336, -0.037793167, 0.026010463, -0.039398175, 0.0391806, 0.030453516, -0.030992106, -0.0077351276, 0.02160491, -0.009477524, -0.010922786, 0.02444803, -0.09086181, 0.067104295, 0.018389635, -0.03606782, 0.07084385, -0.01209742]}]